{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key mathtext.fallback_to_cm in file /home/diego/.config/matplotlib/stylelib/belle2_serif.mplstyle, line 27 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.10.0/lib/matplotlib/mpl-data/matplotlibrc\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /home/diego/.config/matplotlib/stylelib/belle2.mplstyle, line 36 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.10.0/lib/matplotlib/mpl-data/matplotlibrc\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "\n",
    "from utils.get_images import get_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = '../mnist/'\n",
    "x_train_num, y_train_num, x_test_num, y_test_num = get_images(mnist_path)\n",
    "\n",
    "x_train = x_train_num[:50000].reshape(50000, -1).astype(np.float32)\n",
    "y_train = y_train_num[:50000].reshape(50000, 1)\n",
    "\n",
    "x_val = x_train_num[50000:].reshape(10000, -1).astype(float)\n",
    "y_val = y_train_num[50000:].reshape(10000, 1)\n",
    "\n",
    "x_test = x_test_num.copy().reshape(10000, -1).astype(float)\n",
    "y_test = y_test_num.copy().reshape(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x_mean, x_std, x_data):\n",
    "    return (x_data - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se normaliza con los datos de entrenamiento\n",
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "x_train = normalise(x_mean, x_std, x_train)\n",
    "x_val = normalise(x_mean, x_std, x_val)\n",
    "x_test = normalise(x_mean, x_std, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    '''\n",
    "    x  #muestras, 784\n",
    "    y #muestras, 1\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_tensor es una subclase de np.ndarray donde ndarray significa que es un array de numpy\n",
    "class np_tensor(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Init parameters utilizando Kaiming He\n",
    "        '''\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
    "        # .view nos permite hacer una nueva vista, una vista como si fuera np_tensor que creamos\n",
    "        # es decir que estamos permitiendo futuras operaciones que no se pueden hacer en un np.ndarray\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
    "    # __call__ nos permite hacer que la clase Linear() sea llamable y opere como una función\n",
    "    # los guiones abajo sirven para usar la clase como si fuera una función\n",
    "    def __call__(self, X): # esta el foward de la clase lineal\n",
    "        Z = self.W @ X + self.b\n",
    "        return Z\n",
    "    def backward(self, X, Z):\n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        self.W.grad = Z.grad @ X.T # no se divide por el tamaño del batch porque viene integrado en el parámetro alpha\n",
    "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)\n",
    "        # axis = 1 es sumar los elementos de toda la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu():\n",
    "    def __call__(self, z):\n",
    "         return np.maximum(0, z)\n",
    "    def backward(self, z, A):\n",
    "        z.grad = A.grad.copy()\n",
    "        z.grad[z <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementación de una clase que permite hacer capas secunciales\n",
    "class Sequential_layers():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        layers - lista que contiene objetos de tipo linear, ReLu\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.x = None  # almacen de cada capa neuronal\n",
    "        self.outputs = {}\n",
    "    def __call__(self, X):\n",
    "        self.x = X\n",
    "        self.outputs['l0'] = self.x\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            self.x = layer(self.x)\n",
    "            self.outputs[f'l{i}'] = self.x\n",
    "        return self.x # se devuelven los scores, porque el ultimo self.x del for es la salida de la red\n",
    "    def backward(self):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.outputs[f'l{i}'], self.outputs[f'l{i+1}'])\n",
    "    def update(self, learning_rate = 1e-3):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLu): continue\n",
    "        layer.W = layer.W - learning_rate * layer.W.grad\n",
    "        layer.b = layer.b - learning_rate * layer.b.grad\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.__call__(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la función de costo y la de salida softmax se necesitan aun, es común que se creen juntas como funciones\n",
    "\n",
    "def softmaxXEntropy(x, y):\n",
    "    batch_size = x.shape[1]\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
    "\n",
    "    #función de costo\n",
    "    preds = probs.copy()\n",
    "\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "\n",
    "    #calcular gradientes\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1 # se les resta el valor de la clase correcta en en P() = 1\n",
    "    # también da el gradiente dl/dx o dl/dz pero llamamos las entradas x entonces x = z\n",
    "    x.grad = probs.copy()\n",
    "\n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de entrenamiento y accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, mb_size=128, learning_rate=1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            sores = model(x.T.view(np_tensor)) # porque los datos vienen filas elementos y columnas pixeles, entonces hay que trasponer\n",
    "            _, cost = softmaxXEntropy(sores, y)\n",
    "            model.backward()\n",
    "            model.update(learning_rate)\n",
    "        print(f'costo: {cost}, accuracy: {accuracy(x_val, y_val, mb_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y, shuffle = False)):\n",
    "        pred = model(x.T.view(np_tensor))\n",
    "        correct += np.sum(np.argmax(pred, axis = 0) == y.squeeze())\n",
    "        total += pred.shape[1]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential_layers([Linear(784, 100), ReLu(), Linear(100, 10)])\n",
    "# model = Sequential_layers([Linear(784, 200), ReLu(), Linear(200, 10)]) # la capa de salida del primer lineal debe ser igual a la entrada de la segunda\n",
    "# model = sequential_layers([Linear(784, 200), ReLu(), Linear(200, 100), ReLu(), Linear(100, 10)]) # se puede agregar más capas\n",
    "\n",
    "mb_size = 512\n",
    "learning_rate = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo: 1.056051624448978, accuracy: 0.7039\n",
      "costo: 0.8730770898867061, accuracy: 0.7858\n",
      "costo: 0.7668047069921643, accuracy: 0.8122\n",
      "costo: 0.6195161137057745, accuracy: 0.8274\n",
      "costo: 0.5743123216999518, accuracy: 0.8373\n",
      "costo: 0.666247267066232, accuracy: 0.8443\n",
      "costo: 0.5219164656048503, accuracy: 0.8483\n",
      "costo: 0.6098911179745042, accuracy: 0.8515\n",
      "costo: 0.5522869614325185, accuracy: 0.8572\n",
      "costo: 0.5889814727582531, accuracy: 0.8594\n",
      "costo: 0.5334419056306672, accuracy: 0.8621\n",
      "costo: 0.5699663825872573, accuracy: 0.8645\n",
      "costo: 0.5827004806807575, accuracy: 0.8653\n",
      "costo: 0.5163838569496995, accuracy: 0.8686\n",
      "costo: 0.5237256923243473, accuracy: 0.869\n",
      "costo: 0.5178362053521035, accuracy: 0.8692\n",
      "costo: 0.5339459406334448, accuracy: 0.8699\n",
      "costo: 0.5123195181774844, accuracy: 0.8705\n",
      "costo: 0.5229138817276893, accuracy: 0.8726\n",
      "costo: 0.6015344225653584, accuracy: 0.8731\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación con los datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8634\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(x_test, y_test, mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(image):\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor predicho es: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACEVJREFUeJzt3D1rlFkcxuFnlvEFv4AWgqQLiBE0xkB8QUWwESut7bQRwXwJsbHQJn6AgCCIlYWktIhvhUWQECwSFVJpLZhnC+XeYndh/g/OZDZ7XfXczAF1fpzC02vbtm0AoGmaP7b7AACMD1EAIEQBgBAFAEIUAAhRACBEAYAQBQCiP+gHe73eMM8BwJAN8n+V3RQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACD6230AYHjW1tbKm3v37pU3jx49Km8YT24KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOFBPDq5fv16efP58+dO3/XixYtOu51menq6vJmYmChv5ubmyhsP4u0cbgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhFdSaa5evVredHkV8/z58+UNfzlx4kR50+v1ypv19fXyhp3DTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgem3btgN9sMPDWozevn37ypvl5eXy5uDBg+XNkSNHypumaZpPnz512u00S0tL5c3k5GR5Mzs7W95sbGyUN4zeID/3bgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA0d/uA/B73b59u7w5fPhwebOwsFDeeNjup5mZmU6706dPlzcfPnwobzxu9//mpgBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQHsTbYfbv31/efP36tbx58OBBecNPe/fu7bTr9/1zZfjcFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIzy6Oqd27d3faXblypbxZXFwsb1ZWVsobfrp48eLIvqvLy6pdXtrd3NwsbxhPbgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4UG8MTU1NdVpd+jQofLm27dvnb6Lptm1a1d5Mzs7O4ST/LPJycny5vnz5+XNsWPHyhvGk5sCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQHgQb0y1bdtpt7W1Vd5cvny5vHn48GF5s7m5Wd50tWfPnvLm1KlT5c2NGzfKmwsXLpQ3TdPtz3Z1dbW8OXPmTHnDzuGmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABC9dsCX13q93rDPwm+wtLRU3pw7d668WVtbK2+ePXtW3jRN07x9+7a8uXPnTnkzPT1d3ozSkydPyptr164N4ST8Vw3yc++mAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAexNthpqamypuFhYXy5uTJk+VNV13+7i0vL5c3T58+LW8uXbpU3pw9e7a8aZqmmZ+fL2/u37/f6bvYmTyIB0CJKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgBEf7sPwO/1/v378ubmzZvlza1bt8qbjY2N8qZpmubjx4/lzeLiYnnz48eP8qbri6ddvHr1amTfxf+XmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBA9Nq2bQf6YK837LPAtjp69Gh58/r16/JmeXm5vGmabo/vbW1tdfoudqZBfu7dFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQCiv90HgHExMTFR3vT79X9Cb968KW+axuN2jIabAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB4EA9GbHV1dbuPAP/KTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgPIgHv8zNzY3ke1ZWVkbyPdCFmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4ZVU+OXly5flzfz8fHlz4MCB8gZGxU0BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIDyIByM2MzPTaff48ePffBL4OzcFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgPAgHvyyvr5e3nz//r28OX78eHkDo+KmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAexINf3r17V97cvXu3vPny5Ut5A6PipgBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBA9Nq2bQf6YK837LMAMESD/Ny7KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQ/UE/2LbtMM8BwBhwUwAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACD+BMKl3qrc/9o/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test_num[idx])\n",
    "\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "\n",
    "print(f\"El valor predicho es: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementó una red neuronal multicapa, utilizando programación orientada a objetos con clases propias.\n",
    "\n",
    "- Se programo la clase lineal.\n",
    "- La clase ReLu.\n",
    "- La clase sequential, softmax y cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.get_images import get_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist path\n",
    "mnist_path = './mnist/'\n",
    "\n",
    "x_train_num, y_train_num, x_test_num, y_test_num = get_images(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tomar los primeros 50k datos y reagrupar en un formato para que sea procesable por el modelo\n",
    "\n",
    "x_train = x_train_num[:50000].reshape(50000, -1).astype(np.float32)/255\n",
    "y_train = y_train_num[:50000].reshape(50000, 1)\n",
    "\n",
    "x_val = x_train_num[50000:].reshape(10000, -1).astype(np.float32)/255\n",
    "y_val = y_train_num[50000:].reshape(10000, 1)\n",
    "\n",
    "x_test = x_test_num.copy().reshape(10000, -1).astype(float)/255\n",
    "y_test = y_test_num.copy().reshape(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^1$ = $W^1X + b^1$\n",
    "\n",
    "$a^1 = ReLu(z^1)$\n",
    "\n",
    "$z^2$ = $W^2a + b^2$\n",
    "\n",
    "$\\hat{y} = \\frac{e^{z^2_k}}{\\sum_je^{z_j}}$\n",
    "\n",
    "$\\mathcal{L}(\\hat{y}^i, y^i) = -y^iln(\\hat{y}^i) = -ln(\\hat{y}^i)$\n",
    "\n",
    "$\\mathcal{J}(w, b) = \\frac{1}{num_samples}\\sum_{i=1}^{num_samples}(-ln(\\hat{y}^i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para hacer eficiente el entrenamiento en las capas de la red, vamos a partir en mini bachs los datos\n",
    "# crearíamos una matriz de 784xm donde m > 1, mayor a 1 dado que enviar m = 1 es enviar un sólo dato\n",
    "# 1 < m < 50.000 se suelen usar potencias de 2 dado que son más eficientes computacionalmente\n",
    "\n",
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    \"\"\"\n",
    "    x # muestras, 784\n",
    "    y #muestras, 1\n",
    "    shuffle: bandera que nos permite activar o no un muestreo aleatorio\n",
    "    \"\"\"\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    total_data =  x.shape[0]\n",
    "\n",
    "    if shuffle:\n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]\n",
    "\n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para inicializar los parámetros, red de dos capas básica\n",
    "\n",
    "def init_parameters(input_size, neurons):\n",
    "    \"\"\"\n",
    "    input_size: number of input elements\n",
    "    neurons: number neurons in layers ex. [200,10], 200 first layer, 10 second layer\n",
    "    \"\"\"\n",
    "\n",
    "    w1 = np.random.randn(neurons[0], input_size) * 0.001 # para inicializarse deben ser números muy pequeños por eso el 0.001\n",
    "    b1 = np.zeros((neurons[0], 1))\n",
    "\n",
    "    w2 = np.random.randn(neurons[1], neurons[0]) * 0.001\n",
    "    b2 = np.zeros((neurons[1], 1))\n",
    "\n",
    "    return {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 784)\n",
      "(200, 1)\n",
      "(10, 200)\n"
     ]
    }
   ],
   "source": [
    "# prueba de init_parameters\n",
    "parameters = init_parameters(28*28, [200, 10])\n",
    "print(parameters['w1'].shape)\n",
    "print(parameters['b1'].shape)\n",
    "print(parameters['w2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^1 = ReLu(z^1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elimina valores negativos y pasa positivos\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^1$ = $W^1X + b^1$\n",
    "\n",
    "$z^2$ = $W^2a + b^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de activación\n",
    "\n",
    "def scores(x, parameters, activation_fcn):\n",
    "    \"\"\"\n",
    "    x: dim #pixeles, #samples\n",
    "    \"\"\"\n",
    "    z1 = parameters['w1'] @ x + parameters['b1'] # @ multiplicacion de matrices\n",
    "    a1 = activation_fcn(z1) # función de activación\n",
    "    z2 = parameters['w2'] @ a1 + parameters['b2']\n",
    "\n",
    "    return z2, z1, a1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba scores\n",
    "scores_res, z1, a1 = scores(x_train[:64].T, parameters, relu) # datos transpuestos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax\n",
    "\n",
    "$\\hat{y} = \\frac{e^{z^2_k}}{\\sum_je^{z_j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    sum_exp_scores = np.sum(exp_scores, axis=0)\n",
    "    probs = exp_scores/sum_exp_scores\n",
    "\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(\\hat{y}^i, y^i) = -y^iln(\\hat{y}^i) = -ln(\\hat{y}^i)$\n",
    "\n",
    "$\\mathcal{J}(w, b) = \\frac{1}{num_samples}\\sum_{i=1}^{num_samples}(-ln(\\hat{y}^i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de perdida\n",
    "\n",
    "def x_entropy(scores, y, batch_size=64):\n",
    "    probs = softmax(scores)\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)] # squeeze toma solo los elementos y no las columnas\n",
    "    cost = np.sum(-np.log(y_hat))/ batch_size # función de costo, promedio de la función de perdida de cada elemento score\n",
    "\n",
    "    return probs, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de retro propagación\n",
    "\n",
    "def backward(probs, x, y, z1, a1, parameters, batch_size=64):\n",
    "    grads = {}\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1 # se toma la prob de la clase correcta 1 es la prob correcta\n",
    "    dz2 = probs.copy()\n",
    "\n",
    "    dw2 = dz2 @ a1.T / batch_size\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True) / batch_size\n",
    "    da1 = parameters['w2'].T @ dz2\n",
    "\n",
    "    dz1 = da1.copy()\n",
    "    dz1[z1 <= 0] = 0\n",
    "    \n",
    "    dw1 = dz1 @ x\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "    assert parameters['w1'].shape == dw1.shape, 'w1 no igual forma'\n",
    "    assert parameters['w2'].shape == dw2.shape, 'w2 no igual forma'\n",
    "    assert parameters['b1'].shape == db1.shape, 'b1 no igual forma'\n",
    "    assert parameters['b2'].shape == db2.shape, 'b2 no igual forma'\n",
    "\n",
    "    grads = {'w1': dw1, 'b1':db1, 'w2':dw2, 'b2':db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba x_entropy\n",
    "y_hat, cost = x_entropy(scores_res, y_train[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n"
     ]
    }
   ],
   "source": [
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba de backward\n",
    "grads = backward(y_hat, x_train[:64], y_train[64], z1, a1, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para medir el desempeño del modelo\n",
    "def accuracy(x_data, y_data, mb_size=64):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x_data, y_data)):\n",
    "        scores_re, z1, a1 = scores(x.T, parameters, relu)\n",
    "        y_hat, cost = x_entropy(scores_re, y, batch_size=len(x)) # el batch_size=len(x) es porque al final no siempre son 64 datos para enviar\n",
    "\n",
    "        correct += np.sum(np.argmax(y_hat) == y.squeeze()) # argmax devuelve el indice del elemento más grande\n",
    "        total += y_hat.shape[1]\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento, epochs: cantidad de veces que hace un recorrido de completar todos los batchs\n",
    "# learning rate es un valor fijo y es un hiperparámetro\n",
    "\n",
    "def train(epochs, parameters, mb_size=64, learning_rate=1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            scores_re, z1, a1 = scores(x.T, parameters, relu)\n",
    "            y_hat, cost = x_entropy(scores_re, y, batch_size=len(x)) # batch_size=len(x) usualmente le último minibatch es menor a 64\n",
    "            grads = backward(y_hat, x, y, z1, a1, parameters, batch_size=len(x))\n",
    "\n",
    "            parameters['w1'] = parameters['w1'] - learning_rate*grads['w1'] # stochastic gradient descent\n",
    "            parameters['b1'] = parameters['b1'] - learning_rate*grads['b1']\n",
    "            parameters['b2'] = parameters['b2'] - learning_rate*grads['b2']\n",
    "            parameters['w2'] = parameters['w2'] - learning_rate*grads['w2']\n",
    "\n",
    "        print(f\"costo es: {cost} y accuracy: {accuracy(x_val, y_val, mb_size)}\")\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forward -> scores\n",
    "2. Función de costo.\n",
    "3. Gradiente de la función de costo con respecto a los parametros.\n",
    "4. Actualizamos los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 512\n",
    "learning_rate = 1e-3 # se empieza por uno grande y se va bajando\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costo es: 0.06620790387917261 y accuracy: 0.0\n",
      "costo es: 0.058113523030671216 y accuracy: 0.002\n",
      "costo es: 0.049208287395802616 y accuracy: 0.0052\n",
      "costo es: 0.05271094139730846 y accuracy: 0.0\n",
      "costo es: 0.04140766157156408 y accuracy: 0.0046\n",
      "costo es: 0.03761817788241574 y accuracy: 0.0049\n",
      "costo es: 0.047769933248469086 y accuracy: 0.0052\n",
      "costo es: 0.032207446464685985 y accuracy: 0.0\n",
      "costo es: 0.05001054709356488 y accuracy: 0.0\n",
      "costo es: 0.06319347581150835 y accuracy: 0.0033\n",
      "costo es: 0.04205720211179779 y accuracy: 0.0\n",
      "costo es: 0.05921171605012281 y accuracy: 0.005\n",
      "costo es: 0.04016397657735117 y accuracy: 0.0088\n",
      "costo es: 0.05866823212596893 y accuracy: 0.0\n",
      "costo es: 0.04455125429560975 y accuracy: 0.0\n",
      "costo es: 0.07288535934970829 y accuracy: 0.0\n",
      "costo es: 0.047052635922516586 y accuracy: 0.0\n",
      "costo es: 0.04063623428319569 y accuracy: 0.0\n",
      "costo es: 0.032524122065041676 y accuracy: 0.0\n",
      "costo es: 0.021954067551534184 y accuracy: 0.0\n",
      "costo es: 0.053838161420106424 y accuracy: 0.0\n",
      "costo es: 0.03318458140712066 y accuracy: 0.0\n",
      "costo es: 0.06938896494317019 y accuracy: 0.0\n",
      "costo es: 0.057089141859251444 y accuracy: 0.0\n",
      "costo es: 0.04263551826496842 y accuracy: 0.0\n",
      "costo es: 0.02262533447001 y accuracy: 0.0\n",
      "costo es: 0.05566957866849989 y accuracy: 0.0\n",
      "costo es: 0.08840575853284173 y accuracy: 0.0\n",
      "costo es: 0.025642246495337334 y accuracy: 0.0\n",
      "costo es: 0.03450987571249663 y accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "parameters = train(epochs=epochs, parameters=parameters, mb_size=mb_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(x_test, y_test, mb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    scores2, _, _ = scores(x, parameters, relu)\n",
    "    return np.argmax(scores2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_number(image):\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor predicho es: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACOBJREFUeJzt3D+ozn8fx/HP5ZxshuOkZOAMBmcwsRJ1UieDAfMZ2JRJTMpiIEpZlCKl7EKnZPBvQyandIajKLEo55QU1729ot99313vb851zs95PObr1ffTcc55nu/g0+v3+/0GAK21dSt9AABWD1EAIEQBgBAFAEIUAAhRACBEAYAQBQBidNAP9nq95TwHAMtskP+r7E0BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBidKUPAGvN+Ph4p92VK1fKm02bNpU3vV6vvHnw4EF58+zZs/KmtdZevnzZacdgvCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoARK/f7/cH+mCHS7JgpRw9erS8OXDgwDKc5J/279/faTcxMfFnD/I/dPlZH/DXyG8WFxfLm9Zau3r1anlz9uzZTs/62wzy7+RNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwSypD0/WWz7t375Y3O3bsKG9GRkbKmy63g3b15s2b8ubz58/lzb59+8qbYX4dXr16Vd50uZl2aWmpvFnt3JIKQIkoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOFCPNqGDRvKmyNHjpQ309PT5U1rrR0+fLjTrmrduvrfSO/fvy9v9uzZU9601trCwkKn3TDMzMyUN9euXev0rPXr15c3ly5dKm/OnDlT3qx2LsQDoEQUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHAh3io1NjbWabd169by5vbt2+XN5ORkedPV27dvy5tv376VN0+ePClvHj58WN7cv3+/vPkbzc7OdtpNTU2VN3Nzc+XNzp07y5vVzoV4AJSIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCjK32AtWDXrl3lzYULFzo9a9++feVNl8sOv379Wt7cuXOnvGmttdOnT5c3Xc7H32uYFzj+23lTACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACDckjoE58+fL2+63HY6TOfOnStvrly58sfPwb/XmzdvOu2mpqb+8En4lTcFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHAhXtHMzEx5s3fv3mU4yX/34sWL8mZ2dra8uXbtWnkDv+ryfddaaydPnvzDJ+FX3hQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAotfv9/sDfbDXW+6zDN3mzZvLmw8fPizDSf7p6dOnnXYHDx4sb5aWljo9C1bC69evy5udO3eWNyMjI+XNajfIr3tvCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxutIHWEnT09PlzYD3B/5mcXGxvDl+/Hh505rL7fj7dfkZ7LJZq7wpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMSavhBvcnJyKM85depUeTM/P78MJwH4/7wpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABBr+pbUmZmZoTzn3bt3Q3kOrJSxsbHy5ubNm52etX379k47BuNNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACB6/X6/P9AHe73lPsvQ/fjxo7wZ8Mv1my9fvpQ3169fL29aa212dra8efz4cadnsfrt3r27vDl48GB5c+LEifJm48aN5c0wjY7+ffeFDvL7y5sCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQKzpC/Gmp6fLm1u3bpU34+Pj5c0wrVtX/9vg3r175c3c3Fx5M0xdvse7XJB45MiR8qa11rZt29ZpV9Xl++Hnz5/lzffv38ub1lp7/vx5eXPo0KHyZmlpqbxZ7VyIB0CJKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgCxpi/E62JsbKy8uXHjRnnT9fKzTZs2lTdbtmwpb7pcBLfaDetCvK4+f/48lOd8/PixvPn06VN5c/HixfKmtdYePXrUaYcL8QAoEgUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgXIj3l5mYmChvjh07Vt7s3r27vFnthnUh3sLCQnnTWmuXL1/utKuan58fynMYPhfiAVAiCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDhllSANcItqQCUiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADE66Af7/f5yngOAVcCbAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAPEfiS80NxonlFEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test_num[idx])\n",
    "\n",
    "pred = predict(x_test[idx].reshape(-1, 1))\n",
    "\n",
    "print(f\"El valor predicho es: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key mathtext.fallback_to_cm in file /home/diego/.config/matplotlib/stylelib/belle2_serif.mplstyle, line 27 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.10.0/lib/matplotlib/mpl-data/matplotlibrc\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /home/diego/.config/matplotlib/stylelib/belle2.mplstyle, line 36 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.10.0/lib/matplotlib/mpl-data/matplotlibrc\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # neural network\n",
    "import torch.nn.functional as F # activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "\n",
    "from utils.get_images import get_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = '../mnist/'\n",
    "\n",
    "x_train_num, y_train_num, x_test_num, y_test_num = get_images(mnist_path)\n",
    "\n",
    "# se convierten las imagenes en vectores\n",
    "x_train = x_train_num[:50000].reshape(50000, -1).astype(np.float32)\n",
    "y_train = y_train_num[:50000].reshape(50000, 1)\n",
    "\n",
    "x_val = x_train_num[50000:].reshape(10000, -1).astype(np.float32)\n",
    "y_val = y_train_num[50000:].reshape(10000, 1)\n",
    "\n",
    "x_test = x_test_num.copy().reshape(10000, -1).astype(float)\n",
    "y_test = y_test_num.copy().reshape(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función que normaliza y nos permite tener una media de 0 y desviación estándar de 1\n",
    "def normalise(x_mean, x_std, x_data):\n",
    "    return (x_data - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "x_train = normalise(x_mean, x_std, x_train)\n",
    "x_val = normalise(x_mean, x_std, x_val)\n",
    "x_test = normalise(x_mean, x_std, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(x, y, mb_size, shuffle = True):\n",
    "    '''\n",
    "    x  #muestras, 784\n",
    "    y #muestras, 1\n",
    "    shuffle: mezclar los datos, que ordene de forma aleatorea los datos\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir los arreglos numpy a Tensores de Pytorch\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train.copy()) # se copia para no modificar el original y no generar warning de sobreescribir\n",
    "y_train_tensor = torch.tensor(y_train.copy())\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val.copy())\n",
    "y_val_tensor = torch.tensor(y_val.copy())\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test.copy())\n",
    "y_test_tensor = torch.tensor(y_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificar GPU disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se esta utilizando cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Se esta utilizando {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de accuracy se debe modificar\n",
    "\n",
    "def accuracy(model, x, y, mb_size):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    model.eval() # aquellas capas que se comportan de manera diferente en entrenamiento y evaluación\n",
    "    model = model.to(device) # donde queremos correr el modelo, si tenemos gpu allí\n",
    "    with torch.no_grad(): # para que no calcule el gradiente de los parámetros, porque es más eficiente\n",
    "        for (xi, yi) in create_minibatches(x, y, mb_size):\n",
    "            xi = xi.to(device, dtype=torch.float32)\n",
    "            yi = yi.to(device, dtype=torch.long)\n",
    "            scores = model(xi) # 1, 10 ... mb_size, 10\n",
    "            _, pred = scores.max(dim=1)\n",
    "            num_correct += (pred == yi.squeeze()).sum() # squeeze para quitar la dimensión de 1 en el vector yi mb_sizex1\n",
    "            num_total += pred.size(0)\n",
    "        return float(num_correct) / num_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, mb_size, epochs=100):\n",
    "    model = model.to(device=device)\n",
    "    for epoch in range(epochs):\n",
    "        for (xi, yi) in create_minibatches(x_train_tensor, y_train_tensor, mb_size):\n",
    "            model.train()\n",
    "            xi = xi.to(device, dtype=torch.float32)\n",
    "            yi = yi.to(device, dtype=torch.long)\n",
    "            scores = model(xi)\n",
    "            # función de costo\n",
    "            cost = F.cross_entropy(scores, yi.squeeze()) # internamente ya se calcula el softmax\n",
    "            optimiser.zero_grad() # para que no acumule los gradientes dado que esta dentro de un for el train\n",
    "            cost.backward()\n",
    "            optimiser.step() # forma de actualizar los parámetros\n",
    "\n",
    "        print(f\"Epoch {epoch} - Cost: {cost.item()} - Val accuracy: {accuracy(model, x_val_tensor, y_val_tensor, mb_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo usando Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = 1000\n",
    "hidden = 1000\n",
    "learning_rate = 5e-2\n",
    "epochs = 100\n",
    "mb_size = 4096 # es bueno en potencias de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 = nn.Sequential(nn.Linear(784, hidden1), nn.ReLU(),\n",
    "#                       nn.Linear(hidden1, hidden), nn.ReLU(),\n",
    "#                       nn.Linear(hidden, 10)\n",
    "#                      )\n",
    "\n",
    "# muchas veces un modelo más grande es más difícil de entrenar y podemos hacer uno más pequeño\n",
    "# aunque un modelo pequeño no significa mejor, porque puede que no tenga la capacidad de aprender\n",
    "\n",
    "model1 = nn.Sequential(nn.Linear(784, 10), nn.ReLU(), nn.Linear(10, 10))\n",
    "\n",
    "# si comento mi modelo anterior, se corre sumando la nueva cantidad de epochs añadidas, al inicio tuve\n",
    "# 50 epochs y cambié este parámetro a 100 entonces se acumulará la corrida del train a 150 epochs\n",
    "# si descomento el model1 y lo vuelvo a correr, se reiniciará el contador de epochs a 0, es decir un modelo nuevo\n",
    "\n",
    "# También, si en las capas intermedias añado más neuronas, el modelo es más complejo y más difícil de sintonizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.SGD(model1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Cost: 0.43019482493400574 - Val accuracy: 0.8916\n",
      "Epoch 1 - Cost: 0.40504902601242065 - Val accuracy: 0.8921\n",
      "Epoch 2 - Cost: 0.4682528078556061 - Val accuracy: 0.8929\n",
      "Epoch 3 - Cost: 0.42822545766830444 - Val accuracy: 0.8928\n",
      "Epoch 4 - Cost: 0.42836496233940125 - Val accuracy: 0.8938\n",
      "Epoch 5 - Cost: 0.4132584035396576 - Val accuracy: 0.8949\n",
      "Epoch 6 - Cost: 0.4283534288406372 - Val accuracy: 0.8943\n",
      "Epoch 7 - Cost: 0.48783594369888306 - Val accuracy: 0.8961\n",
      "Epoch 8 - Cost: 0.3934120535850525 - Val accuracy: 0.8955\n",
      "Epoch 9 - Cost: 0.3931410014629364 - Val accuracy: 0.8955\n",
      "Epoch 10 - Cost: 0.4372955560684204 - Val accuracy: 0.8962\n",
      "Epoch 11 - Cost: 0.4120085835456848 - Val accuracy: 0.8967\n",
      "Epoch 12 - Cost: 0.41573280096054077 - Val accuracy: 0.8973\n",
      "Epoch 13 - Cost: 0.4359157085418701 - Val accuracy: 0.8974\n",
      "Epoch 14 - Cost: 0.43391698598861694 - Val accuracy: 0.898\n",
      "Epoch 15 - Cost: 0.44330212473869324 - Val accuracy: 0.8986\n",
      "Epoch 16 - Cost: 0.3930099904537201 - Val accuracy: 0.8989\n",
      "Epoch 17 - Cost: 0.3882611095905304 - Val accuracy: 0.8986\n",
      "Epoch 18 - Cost: 0.40084967017173767 - Val accuracy: 0.8995\n",
      "Epoch 19 - Cost: 0.45006364583969116 - Val accuracy: 0.9001\n",
      "Epoch 20 - Cost: 0.3578285574913025 - Val accuracy: 0.8999\n",
      "Epoch 21 - Cost: 0.3947284519672394 - Val accuracy: 0.9005\n",
      "Epoch 22 - Cost: 0.4311946928501129 - Val accuracy: 0.9009\n",
      "Epoch 23 - Cost: 0.3927035331726074 - Val accuracy: 0.9011\n",
      "Epoch 24 - Cost: 0.37924841046333313 - Val accuracy: 0.9019\n",
      "Epoch 25 - Cost: 0.37173891067504883 - Val accuracy: 0.902\n",
      "Epoch 26 - Cost: 0.38604357838630676 - Val accuracy: 0.9018\n",
      "Epoch 27 - Cost: 0.34588947892189026 - Val accuracy: 0.9021\n",
      "Epoch 28 - Cost: 0.3985665440559387 - Val accuracy: 0.9019\n",
      "Epoch 29 - Cost: 0.38574859499931335 - Val accuracy: 0.9028\n",
      "Epoch 30 - Cost: 0.3806725740432739 - Val accuracy: 0.9025\n",
      "Epoch 31 - Cost: 0.35694602131843567 - Val accuracy: 0.9027\n",
      "Epoch 32 - Cost: 0.37254223227500916 - Val accuracy: 0.9032\n",
      "Epoch 33 - Cost: 0.37641462683677673 - Val accuracy: 0.9039\n",
      "Epoch 34 - Cost: 0.3705565929412842 - Val accuracy: 0.9038\n",
      "Epoch 35 - Cost: 0.41358715295791626 - Val accuracy: 0.9039\n",
      "Epoch 36 - Cost: 0.3352881968021393 - Val accuracy: 0.9042\n",
      "Epoch 37 - Cost: 0.3436037600040436 - Val accuracy: 0.9041\n",
      "Epoch 38 - Cost: 0.36340779066085815 - Val accuracy: 0.9053\n",
      "Epoch 39 - Cost: 0.3264514207839966 - Val accuracy: 0.9057\n",
      "Epoch 40 - Cost: 0.338901549577713 - Val accuracy: 0.9054\n",
      "Epoch 41 - Cost: 0.3437141180038452 - Val accuracy: 0.9049\n",
      "Epoch 42 - Cost: 0.36485329270362854 - Val accuracy: 0.9056\n",
      "Epoch 43 - Cost: 0.3685535490512848 - Val accuracy: 0.9059\n",
      "Epoch 44 - Cost: 0.3026442527770996 - Val accuracy: 0.906\n",
      "Epoch 45 - Cost: 0.3474524915218353 - Val accuracy: 0.9059\n",
      "Epoch 46 - Cost: 0.30801287293434143 - Val accuracy: 0.9058\n",
      "Epoch 47 - Cost: 0.423157662153244 - Val accuracy: 0.9058\n",
      "Epoch 48 - Cost: 0.3265281617641449 - Val accuracy: 0.9057\n",
      "Epoch 49 - Cost: 0.375738263130188 - Val accuracy: 0.9064\n",
      "Epoch 50 - Cost: 0.3237299621105194 - Val accuracy: 0.9068\n",
      "Epoch 51 - Cost: 0.3553828001022339 - Val accuracy: 0.9069\n",
      "Epoch 52 - Cost: 0.33317968249320984 - Val accuracy: 0.9068\n",
      "Epoch 53 - Cost: 0.3343585431575775 - Val accuracy: 0.9067\n",
      "Epoch 54 - Cost: 0.3508690893650055 - Val accuracy: 0.9067\n",
      "Epoch 55 - Cost: 0.3291160762310028 - Val accuracy: 0.9067\n",
      "Epoch 56 - Cost: 0.3786773979663849 - Val accuracy: 0.9071\n",
      "Epoch 57 - Cost: 0.32590845227241516 - Val accuracy: 0.9073\n",
      "Epoch 58 - Cost: 0.3474423885345459 - Val accuracy: 0.9072\n",
      "Epoch 59 - Cost: 0.35029104351997375 - Val accuracy: 0.9069\n",
      "Epoch 60 - Cost: 0.3570808172225952 - Val accuracy: 0.9076\n",
      "Epoch 61 - Cost: 0.3853074908256531 - Val accuracy: 0.9076\n",
      "Epoch 62 - Cost: 0.3387852907180786 - Val accuracy: 0.9081\n",
      "Epoch 63 - Cost: 0.44641757011413574 - Val accuracy: 0.908\n",
      "Epoch 64 - Cost: 0.3746606707572937 - Val accuracy: 0.9076\n",
      "Epoch 65 - Cost: 0.34507307410240173 - Val accuracy: 0.9082\n",
      "Epoch 66 - Cost: 0.3571094274520874 - Val accuracy: 0.9079\n",
      "Epoch 67 - Cost: 0.3295401334762573 - Val accuracy: 0.9087\n",
      "Epoch 68 - Cost: 0.35245996713638306 - Val accuracy: 0.9082\n",
      "Epoch 69 - Cost: 0.3356703221797943 - Val accuracy: 0.9083\n",
      "Epoch 70 - Cost: 0.34565243124961853 - Val accuracy: 0.9086\n",
      "Epoch 71 - Cost: 0.29486072063446045 - Val accuracy: 0.9086\n",
      "Epoch 72 - Cost: 0.30964699387550354 - Val accuracy: 0.9086\n",
      "Epoch 73 - Cost: 0.33222126960754395 - Val accuracy: 0.9088\n",
      "Epoch 74 - Cost: 0.3516499102115631 - Val accuracy: 0.9087\n",
      "Epoch 75 - Cost: 0.32070833444595337 - Val accuracy: 0.9087\n",
      "Epoch 76 - Cost: 0.36129528284072876 - Val accuracy: 0.9088\n",
      "Epoch 77 - Cost: 0.3348337709903717 - Val accuracy: 0.9089\n",
      "Epoch 78 - Cost: 0.29301953315734863 - Val accuracy: 0.9095\n",
      "Epoch 79 - Cost: 0.37904396653175354 - Val accuracy: 0.9092\n",
      "Epoch 80 - Cost: 0.2992931306362152 - Val accuracy: 0.9096\n",
      "Epoch 81 - Cost: 0.32340672612190247 - Val accuracy: 0.91\n",
      "Epoch 82 - Cost: 0.3029974400997162 - Val accuracy: 0.9102\n",
      "Epoch 83 - Cost: 0.3727656900882721 - Val accuracy: 0.9101\n",
      "Epoch 84 - Cost: 0.30252090096473694 - Val accuracy: 0.9104\n",
      "Epoch 85 - Cost: 0.32328760623931885 - Val accuracy: 0.9108\n",
      "Epoch 86 - Cost: 0.36283835768699646 - Val accuracy: 0.9106\n",
      "Epoch 87 - Cost: 0.3645411431789398 - Val accuracy: 0.9106\n",
      "Epoch 88 - Cost: 0.35778138041496277 - Val accuracy: 0.9107\n",
      "Epoch 89 - Cost: 0.3683522343635559 - Val accuracy: 0.9106\n",
      "Epoch 90 - Cost: 0.3377075791358948 - Val accuracy: 0.9113\n",
      "Epoch 91 - Cost: 0.33886513113975525 - Val accuracy: 0.9111\n",
      "Epoch 92 - Cost: 0.25728729367256165 - Val accuracy: 0.9112\n",
      "Epoch 93 - Cost: 0.3027441203594208 - Val accuracy: 0.911\n",
      "Epoch 94 - Cost: 0.3466731309890747 - Val accuracy: 0.9117\n",
      "Epoch 95 - Cost: 0.3194396197795868 - Val accuracy: 0.9126\n",
      "Epoch 96 - Cost: 0.2891179919242859 - Val accuracy: 0.9119\n",
      "Epoch 97 - Cost: 0.32216355204582214 - Val accuracy: 0.9117\n",
      "Epoch 98 - Cost: 0.36280664801597595 - Val accuracy: 0.9124\n",
      "Epoch 99 - Cost: 0.3468274772167206 - Val accuracy: 0.9121\n"
     ]
    }
   ],
   "source": [
    "train(model1, optimiser, mb_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9082"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model1, x_test_tensor, y_test_tensor, mb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este accuracy con un modelo sencillo en Pytorch sin tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
